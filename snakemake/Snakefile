from snakemake.utils import min_version
min_version("5.17.0")

configfile: "config.yaml"
#container: "envs/hand_sandbox.simg"  # only effective when --use-singularity
# removed gloabal singularity so that can use --use-singularity and --use-conda at the same time

#localrules: targets, create_dag

import pandas as pd
import sys
import re
import os
import math
import gzip
import shutil

def gunzip(fname):
    outname = re.sub(".gz$", "", fname)
    if outname == fname:
        sys.exit("config error:", fname, "does not end with .gz")
    # todo: skip if uncompressed file exists (but how to check for previous uncompress integrity
    # todo: use bash gunzip to keep system timestamp of gz file
    with gzip.open(fname, 'rb') as f_in:
        with open(outname, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)


# umcompress files
if config["GENOME"].endswith(".gz"):
    gunzip(config["GENOME"])
    config["GENOME"] = re.sub(".gz$", "", config["GENOME"])

if config["GTF"].endswith(".gz"):
    gunzip(config["GTF"])
    config["GTF"] = re.sub(".gz$", "", config["GTF"])

if config["VCF"].endswith(".gz"):
    gunzip(config["VCF"])
    config["VCF"] = re.sub(".gz$", "", config["VCF"])


# params
SAMPLES=config['SAMPLES']
GENOME=config["GENOME"]
INDEX=config["INDEX"]
GTF=config["GTF"]
ANNO_TAB=config["ANNO_TAB"]
STRAND=config["STRAND"]
MODE=config["MODE"]
MAX_FDR=config["MAX_FDR"]
MIN_LFC=config["MIN_LFC"]
MIN_GENE_COUNT= config["MIN_GENE_COUNT"] if 'MIN_GENE_COUNT' in config else 100  # DEXSeq




# priority 0-100, default 0

# get CONTRASTS from meta/contrast.xlsx


# Get contrast name for DESeq2
def get_contrast_fnames (fname):
    df=pd.read_excel(fname)
    CONTRASTS = []
    for j in range(df.shape[1]):
        c1 = df.iloc[0,j]
        c2 = df.iloc[1,j]
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1 = re.sub(";", ".", c1)  # remove extra ;
        c2 = re.sub(";", ".", c2)
        CONTRASTS.append(c1 + "_vs_" + c2)
    # CONTRATS = ['KO_D8_vs_KO_D0', 'WT_D8.KO_D8_vs_WT_D0.KO_D0', 'KO_D0.KO_D2.KO_D8_vs_WT_D0.WT_D2.WT_D8']
    return (CONTRASTS)


def get_contrast_groups (fname):
    df2=pd.read_excel(fname)
    C1S = []; C2S = []
    for j in range(df2.shape[1]):
        c1 = df2.iloc[0, j].strip()
        c2 = df2.iloc[1, j].strip()
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1s = c1.split(";")
        c2s = c2.split(";")
        C1S.append(c1s)
        C2S.append(c2s)
    return ([C1S, C2S])


def get_dict_from_meta (fname):
    df = pd.read_excel(fname)
    d = {}
    for i in range(df.shape[0]):
        sample = df.iloc[i, 0]
        group = df.iloc[i, 1]
        if group in d:
            d[group].append(sample)
        else:
            d[group] = []
            d[group].append(sample)
    return (d)


def swap_list_element(l, d):
    return [swap_list_element(x, d) if isinstance(x, list) else d.get(x, x) for x in l]
    
def flatten(l):
    flat_list = [item for sublist in l for item in sublist]
    return (flat_list)

def collapseL2(L3):
    L2 = []
    for l in L3:
        L2.append(flatten(l))
    return (L2)

def s2b(S):
    K = flatten(S)
    V = ["sorted_reads/"+x+".bam" for x in K]
    return (dict(zip(K, V)))

def G2B_workflow(G, g2s):
    S = swap_list_element(G, g2s)
    S = collapseL2(S)
    s2b_dict = s2b(S)
    BS = swap_list_element(S, s2b_dict)
    B = [",".join(x) for x in BS]
    return (BS, B)



# for DESeq2
if config['DESEQ2_ANALYSIS'] and config['START'] in ["FASTQ", "BAM", "COUNT"]:
    CONTRASTS_DE = get_contrast_fnames(config['CONTRAST_DE'])
    #print("CONTRASTS_DE:\n", CONTRASTS_DE)
else:
    CONTRASTS_DE = ["placeholder"]

# for DEXSeq
if config['DEXSEQ_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]:
    CONTRASTS_AS = get_contrast_fnames(config['CONTRAST_AS'])
else:
    CONTRASTS_AS = ["placeholder"]
CONTRASTS_AS = [l.replace('.','_') for l in CONTRASTS_DE]


# for rMATS
if config['RMATS_ANALYSIS'] and config['START'] in ["FASTQ", "BAM"]:
    G = get_contrast_groups(config['CONTRAST_AS'])
    #print("\nGRPUPS:\n", G[0], "\nVS\n", G[1])

    g2s = get_dict_from_meta(config['META'])
    #print("\nGroup To Sample Mapping:\n", g2s)

    B1S, B1 = G2B_workflow(G[0], g2s)
    B2S, B2 = G2B_workflow(G[1], g2s)

    # print("\nrMATS -b1:\n", B1)
    # print("B1S:", B1S)
    # print("rMATS -b2:\n", B2)
    # print("B2S:", B2S)

    # Get contrast for rMATS
    df2 = pd.read_excel(config['CONTRAST_AS'])
    ASCN = df2.shape[1] # alternative splicing contrast count
    #print("ASCN", ASCN)
else:
    B1 = "placeholder.bam"
    B1S = ["placeholder"]
    B2 = B1
    B2S = B1S
    ASCN = 0



# load modules (have to use """, to keep in one block)
# - alias does not work, have to use $samstat
shell.prefix("""
            # module load star/2.5.3a
            # module load singularity/singularity-current
            # conda activate osr  # have to create conda env osr first
            """)

# Requirements
# inputs in ./fastq/xxx.{fastq,fq}.gz
# named as {sample}.{R1,R2}.{fastq,fq}.gz

# SnakeMake Coding Notes:
# input don't have to be used, just for draw nice DAG
# 07/10/2019 randomized primary alignment

ruleorder: create_dag > DESeq2 > GSEA  > rMATS > FastQC > bam_qc > bamCoverage > feature_count  > samtools_sort > samtools_index > reset

def GSEA_OUTPUT(config):
    if config["GSEA_ANALYSIS"]:
        if config["START"] in ["FASTQ", "BAM", "COUNT"]:
            return(expand("gsea/{contrast}/{db}.GseaPreranked/index.html", contrast=CONTRASTS_DE, db=config["GSEA_DBS"]))
        else:
            return(expand("gsea/{contrast}/{db}.GseaPreranked/index.html", contrast=config["RNKS"], db=config["GSEA_DBS"]))
    else:
        return("Workflow_DAG.all.svg")

rule targets:
    input:
        # 1. everything listed here will be produced by the pipeline
        # 2. feed {sample}
        fastqc=("fastqc/multiqc_report.html" 
            if config["START"] == "FASTQ" 
            else "Workflow_DAG.all.svg"),

        STAR_Align_multiqc=("bam_qc/STAR_Align_summary_multiqc_report.html"
            if config["START"] == "FASTQ" # todo: only when STAR used as aligner
            else "Workflow_DAG.all.svg"),

        bam_qc=(expand("bam_qc/idxstats/{sample}.idxstats.txt", sample=SAMPLES) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        bam_qc_multiqc=("bam_qc/samtools_stats_multiqc_report.html"
            if config['START'] in ['FASTQ', 'BAM']
            else 'Workflow_DAG.all.svg'),

        QoRTs=(expand("bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK", sample=SAMPLES)
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        QoRTs_MultiPlot=("bam_qc/QoRTs_MultiPlot/plot-basic.pdf"
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        bamCoverage=(expand("bigWig/{sample}.{mode}.cpm.bw", sample=SAMPLES, mode=MODE) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        feature_count_strict=(expand("feature_count/counts.s{strand}.strict.txt", strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "strict"
            else "Workflow_DAG.all.svg"),

        feature_count_liberal=(expand("feature_count/counts.s{strand}.liberal.txt", strand=STRAND)       
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "liberal"
            else "Workflow_DAG.all.svg"),

        feature_count_gene_strict=(expand('feature_count_gene_level/counts.s{strand}.gene_level.strict.txt', strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "strict" and config['INTRON']
            else "Workflow_DAG.all.svg"),

        feature_count_gene_liberal=(expand('feature_count_gene_level/counts.s{strand}.gene_level.liberal.txt', strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "liberal" and config['INTRON']
            else "Workflow_DAG.all.svg"),

        DESeq2="DESeq2/DESeq2.html"
            if config["START"] in ["FASTQ", "BAM", "COUNT"] and config['DESEQ2_ANALYSIS']
            else "Workflow_DAG.all.svg",

        GSEA = GSEA_OUTPUT(config),

        rMATS=expand("rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt", ascn=range(1, 1+ASCN))
            if config['RMATS_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"] 
            else "Workflow_DAG.all.svg",

        DEXSeq_count=expand("DEXSeq_count/{sample}_count.txt", sample=SAMPLES)
            if config['DEXSEQ_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg",

        DEXSeq=expand("DEXSeq/contrast{ascn}/contrast{ascn}.RData", ascn=range(1,ASCN+1))
            if config['DEXSEQ_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg",

        SalmonTE=("SalmonTE_output/EXPR.csv"
            if config['TE_ANALYSIS'] and config["START"] == "FASTQ"
            else "Workflow_DAG.all.svg"),

        #CUFFMERGE='cuffmerge/merged.gtf' if config["ASSEMBLY_ANALYSIS"] and config["START"] in ["FASTQ"] else "Workflow_DAG.all.svg",

        stringtie='stringtie/stringtie.merged.gtf' if config["ASSEMBLY_ANALYSIS"] and config["START"] in ["FASTQ"] else "Workflow_DAG.all.svg",

        GATK_ASE=(expand("GATK_ASEReadCounter/{sample}.table", sample=SAMPLES)
            if config["ASE_ANALYSIS"] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg"),

        dag="Workflow_DAG.all.svg", # create DAG
#        organize_results="log/oranize_results.finished",
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000   

## fastqc
if config["PAIR_END"]:
    rule FastQC:
        # don't need input, if you agree on not checking them
        # without output, output will not be created
        input:
            r1="fastq/{sample}.R1.fastq.gz",
            r2="fastq/{sample}.R2.fastq.gz",
        output:
            r1="fastqc/details/{sample}.R1_fastqc.html", 
            r2="fastqc/details/{sample}.R2_fastqc.html" 
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/fastqc/{sample}.log"
        benchmark:
            "log/fastqc/{sample}.tsv"
        container:
            "envs/hand_sandbox.simg"
        envmodules:
            "fastqc/0.11.5"
        shell:
            "mkdir -p fastqc && mkdir -p fastqc/details; "
            "which fastqc &> {log};"
            "fastqc -t {threads} {input} -o fastqc/details &>> {log};"
else:
    rule FastQC:
        # don't need input, if you agree on not checking them
        # without output, output will not be created
        input:
            r1="fastq/{sample}.fastq.gz" 
        output:
            "fastqc/details/{sample}_fastqc.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/fastqc/{sample}.log"
        benchmark:
            "log/fastqc/{sample}.tsv"
        container:
            "envs/hand_sandbox.simg"
        envmodules:
            "fastqc/0.11.5"
        shell:
            "mkdir -p fastqc && mkdir -p fastqc/details; "
            "which fastqc &> {log};"
            "fastqc -t {threads} {input} -o fastqc/details &>> {log};"


# rule MultiQC
if config["PAIR_END"]:
    rule MultiQC:
        input:
            r1=expand("fastqc/details/{sample}.R1_fastqc.html", sample=SAMPLES),
            r2=expand("fastqc/details/{sample}.R2_fastqc.html", sample=SAMPLES) 
        output:
            "fastqc/multiqc_report.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/multiqc/multiqc.log"
        benchmark:
            "log/multiqc/multiqc.tsv"
        container:
            "envs/hand_sandbox.simg"
        shell:
            "which multiqc &> {log};"
            "rm -rf fastqc/multiqc_data && multiqc fastqc/details -o fastqc &>> {log};"
            "D=fastqc; rm  -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};"
else:
    rule MultiQC:
        input:
            r1=expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES),
            r2=expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES), # trick snakemake to skip r2
        output:
            "fastqc/multiqc_report.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/multiqc/multiqc.log"
        benchmark:
            "log/multiqc/multiqc.tsv"
        container:
            "envs/hand_sandbox.simg"
        shell:
            "which multiqc &> {log};"
            "rm -rf fastqc/multiqc_data && multiqc fastqc/details -o fastqc &>> {log};"
            "D=fastqc; rm  -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};"


rule star_idx:
    input:
        fa=GENOME,
        gtf=GTF,
    output:
        INDEX+"/SAindex"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 3200   
    threads:
        12
    log:
        "log/star_idx/star_idx.log"
    benchmark:
        "log/star_idx/star_idx.tsv"
    envmodules:
        "star/2.7.5a"
    shell:
        """
        #mkdir -p {INDEX}
        whoami > {log};
        which STAR >> {log};

        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {INDEX} \
        --genomeFastaFiles {input.fa} \
        --sjdbGTFfile {input.gtf} &>> {log}
        """


# rule STAR_Align
if config['PAIR_END']:
    rule STAR_Align:
        input:
            index=INDEX+"/SAindex",
            gtf=GTF,
            r1="fastq/{sample}.R1.fastq.gz",
            r2="fastq/{sample}.R2.fastq.gz"
        output:
            log="mapped_reads/{sample}.Log.final.out",
            bam=temp("mapped_reads/{sample}.bam"),
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 3000 
        params:
            reads="fastq/{sample}.R1.fastq.gz fastq/{sample}.R2.fastq.gz"
        threads:
            12
        log:
            "log/mapped_reads/{sample}.star.log"
        benchmark:
            "log/mapped_reads/{sample}.star.tsv"
        envmodules:
            "star/2.7.5a"
        shell:
            """
            which STAR > {log};
            STAR --runThreadN {threads} \
            --genomeDir {INDEX} \
            --sjdbGTFfile {input.gtf} \
            --readFilesCommand zcat \
            --readFilesIn {params.reads} \
            --outFileNamePrefix mapped_reads/{wildcards.sample}. \
            --outFilterType BySJout \
            --outMultimapperOrder Random \
            --outFilterMultimapNmax 200 \
            --alignSJoverhangMin 8 \
            --alignSJDBoverhangMin 3 \
            --outFilterMismatchNmax 999 \
            --outFilterMismatchNoverReadLmax 0.05 \
            --alignIntronMin 20 \
            --alignIntronMax 1000000 \
            --alignMatesGapMax 1000000 \
            --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
            --outSAMstrandField None \
            --outSAMtype BAM Unsorted \
            --quantMode GeneCounts \
            --outReadsUnmapped Fastx \
            >> {log} 2>&1

            mv mapped_reads/{wildcards.sample}.Aligned.out.bam mapped_reads/{wildcards.sample}.bam &>> {log}
            pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*  &>> {log}
            """
else:
    rule STAR_Align:
        input:
            index=INDEX+"/SAindex",
            gtf=GTF,
            r1="fastq/{sample}.fastq.gz",
        output:
            temp("mapped_reads/{sample}.bam"),
            "mapped_reads/{sample}.Log.final.out"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 3000   ,
        params:
            reads="fastq/{sample}.fastq.gz",
        threads:
            12
        log:
            "log/mapped_reads/{sample}.star.log"
        benchmark:
            "log/mapped_reads/{sample}.star.tsv"
        envmodules:
            "star/2.7.5a"
        shell:
            """
            # todo: documentation for liberal mode: STAR->featureCounts
            which STAR > {log};
            STAR --runThreadN {threads} \
            --genomeDir {INDEX} \
            --sjdbGTFfile {input.gtf} \
            --readFilesCommand zcat \
            --readFilesIn {params.reads} \
            --outFileNamePrefix mapped_reads/{wildcards.sample}. \
            --outFilterType BySJout \
            --outMultimapperOrder Random \
            --outFilterMultimapNmax 200 \
            --alignSJoverhangMin 8 \
            --alignSJDBoverhangMin 3 \
            --outFilterMismatchNmax 999 \
            --outFilterMismatchNoverReadLmax 0.05 \
            --alignIntronMin 20 \
            --alignIntronMax 1000000 \
            --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
            --outSAMstrandField None \
            --outSAMtype BAM Unsorted \
            --quantMode GeneCounts \
            --outReadsUnmapped Fastx \
            >> {log} 2>&1

            mv mapped_reads/{wildcards.sample}.Aligned.out.bam mapped_reads/{wildcards.sample}.bam
            pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*
            """

rule STAR_Align_multiqc:
    input:
        expand("mapped_reads/{sample}.Log.final.out", sample=SAMPLES)
    output:
        expand("bam_qc/STAR_Align_summary/{sample}.Log.final.out", sample=SAMPLES),
        "bam_qc/STAR_Align_summary_multiqc_report.html",    
    resources:
        mem_mb="2000"
    threads:
        1
    log:
        'log/bam_qc/STAR_Align_summary_multiqc_report.log'
    benchmark:
        'log/bam_qc/STAR_Align_summary_multiqc_report.tsv'
    shell:
        """
        mkdir -p bam_qc/
        mkdir -p bam_qc/STAR_Align_summary/
        cp {input} bam_qc/STAR_Align_summary/

        multiqc {input}  --outdir bam_qc --title STAR_Align_summary
        """


rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000 
    threads:
        4
    container:
        "envs/hand_sandbox.simg" # 2.7.1 does not work for small fa, also too old index version
    envmodules:
        "samtools/1.9"
    log:
        "log/samtools_sort/{sample}.sort.log"
    benchmark:
        "log/samtools_sort/{sample}.sort.tsv"
    shell:
        "which samtools &> {log};"
        "samtools sort -@ {threads} -m 3G {input} -o {output} &>> {log}"


rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        1
    log:
        "log/samtools_index/{sample}.index.log"
    benchmark:
        "log/samtools_index/{sample}.index.tsv"
    container:
        "envs/hand_sandbox.simg" 
    envmodules:
        "samtools/1.9"
    shell:
        "which samtools &> {log};"
        "samtools index -@ {threads} {input} &>> {log}"

rule bam_qc:
    """
    with samtools
    """
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        stats="bam_qc/stats/{sample}.stats.txt",
        idxstats="bam_qc/idxstats/{sample}.idxstats.txt",
    container:
        "envs/hand_sandbox.simg" # not necessary after samstat disabled
    priority:
        0
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        2
    log:
        idxstats="log/bam_qc/idxstats/{sample}.idxstats.log",
        stats="log/bam_qc/stats/{sample}.stats.log",
    benchmark:
        "log/bam_qc/benchmark/{sample}.bam_qc.tsv",
    shell:
        """
        mkdir -p bam_qc 
        mkdir -p bam_qc/idxstats
        mkdir -p bam_qc/stats

        samtools idxstats {input.bam} > {output.idxstats} 2> {log.idxstats} &
        samtools stats {input.bam} > {output.stats} 2> {log.stats} &

        wait
        """

rule bam_qc_multiqc:
    input:
        stats=expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES),
        idxstats=expand("bam_qc/idxstats/{sample}.idxstats.txt", sample=SAMPLES)
    output:
        "bam_qc/samtools_stats_multiqc_report.html",    
        "bam_qc/samtools_idxstats_multiqc_report.html",   
    resources:
        mem_mb="2000"
    threads:
        1
    log:
        'log/bam_qc/samtools_multiqc_report.log'
    benchmark:
        'log/bam_qc/samtools_multiqc_report.tsv'
    shell:
        """
        multiqc {input.stats}  --outdir bam_qc --title samtools_stats &> {log}
        multiqc {input.idxstats}  --outdir bam_qc --title samtools_idxstats &> {log}
        """



rule CUFFLINKS_FLT: # SKipped in standard analysis, because STRINGTIE performs better
    '''
    MQ > 30; 
    More than 35BP M

    '''
    input:
        'sorted_reads/{sample}.bam'
    output:
        bam='filtered_reads/{sample}.bam',
        bai='filtered_reads/{sample}.bam.bai'
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2200
    threads:
        4
    params:
        # todo: parameter of MIN_MAPPED_LEN
        MIN_M="perl -lane 'if (/^@/){print;next}; @M=$F[5]=~/(\d+)M/g; foreach(@M){$sum+=$_};print if $sum > 35;$sum=0'", 
        FIX_XS="perl script/bam_xs_fix.pl"
    log:
        "log/filtered_reads/{sample}.gsnap_flt.log"
    benchmark:
        "log/filtered_reads/{sample}.gsnap_flt.tsv"
    shell:
        """
        # Filter, sort
        # todo: PE and SE MODE
        # todo: STAR XS parameter
        samtools view -hq 30 {input} 2>{log} | \
        samtools view -hf 0x2  2>{log} | \
        {params.MIN_M} 2>{log} | \
        {params.FIX_XS} 2>{log} | \
        samtools sort -m 2G -@ {threads} -O BAM -o {output.bam} &>{log} ; 

        samtools index -@ {threads} {output.bam} &>{log}
        """


rule CUFFLINKS:  # SKipped in standard analysis, because STRINGTIE performs better
    input:
        # todo: by groups
        "filtered_reads/{sample}.bam"
    output:
        "cufflinks/{sample}/transcripts.gtf"
    params:
        ref=config['GTF'] 
    envmodules:
        "cufflinks/2.2.1"
    resources: 
        mem_mb=lambda wildcards, attempt: attempt * 3000   , # todo: reduce
    threads:
        6
    log:
        "log/cufflinks/{sample}.cufflinks.log"
    benchmark:
        "log/cufflinks/{sample}.cufflinks.tsv"
    shell:
        """
        cufflinks -o cufflinks/{wildcards.sample} -p {threads} -q \
        -F 0.2 -j 0.3 -a 0.0001 -A 0.4 \
        --min-frags-per-transfrag 20 \
        --overhang-tolerance 20 \
        --overlap-radius 50 \
        --3-overhang-tolerance 50 \
        -g {params.ref} {input} &> {log}
        cat {output}|sort -k1,1 -k4,4n > cufflinks/{wildcards.sample}/transcripts.sort.gtf
        """ 


rule CUFFMERGE:  # SKipped in standard analysis, because STRINGTIE performs better
    input:
        expand("cufflinks/{sample}/transcripts.gtf", sample=SAMPLES)
    output:
        "cuffmerge/merged.gtf"
    params:
        ref=config['GTF'],
        convert='sed "s/ /\\n/g"'
    envmodules:
        "cufflinks/2.2.1 python/2.7.14"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 3000   ,
    threads:
        4
    log:
        "log/cufflinks/cuffmerge.log"
    benchmark:
        "log/cufflinks/cuffmerge.tsv"
    shell:
        """
        mkdir -p cuffmerge/
        echo {input} | {params.convert} > cuffmerge/assembly_list.txt 2>> {log}
        cuffmerge -o cuffmerge -g {params.ref} -p {threads} --min-isoform-fraction 0 cuffmerge/assembly_list.txt &>> {log}
        cat {output}| sort -k1,1 -k4,4n > cuffmerge/merged.sort.gtf 2>> {log}
        """



# rule QoRTs
if config['PAIR_END']:
    rule QoRTs:
        input:
            bam="sorted_reads/{sample}.bam",
            gtf=config['GTF']
        output:
            "bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 16500
        threads:
            1
        log:
            "log/QoRTs/QoRTs.{sample}.log"
        benchmark:
            "log/QoRTs/QoRTs.{sample}.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # file access issue
        shell:
            """
            mkdir -p bam_qc/
            mkdir -p bam_qc/QoRTs/
            which java &> {log}
            java -Xmx16G -jar envs/hartleys-QoRTs-099881f/QoRTs.jar QC \
            {input.bam} {input.gtf} bam_qc/QoRTs/{wildcards.sample}/ &>> {log}
            """
else:
    rule QoRTs:
        input:
            bam="sorted_reads/{sample}.bam",
            gtf=config['GTF']
        output:
            "bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 16500
        threads:
            1
        log:
            "log/QoRTs/QoRTs.{sample}.log"
        benchmark:
            "log/QoRTs/QoRTs.{sample}.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # file access issue
        shell:
            """
            mkdir -p bam_qc/
            mkdir -p bam_qc/QoRTs/
            which java &> {log}
            java -Xmx16G -jar envs/hartleys-QoRTs-099881f/QoRTs.jar QC --singleEnded \
            {input.bam} {input.gtf} bam_qc/QoRTs/{wildcards.sample}/ &>> {log}
            """


rule QoRTs_MultiPlot:
    input:
        expand("bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK", sample=SAMPLES)
    output:
        "bam_qc/QoRTs_MultiPlot/plot-basic.pdf"
    container:
        "envs/hand_sandbox.simg"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000   ,
    threads:
        2
    log:
        "log/QoRTs/multiplot.log"
    benchmark:
        "log/QoRTs/multiplot.tsv"
    shell:
        """
        which python && which Rscript &> {log};
        python script/meta_to_decoder.py &>> {log};
        Rscript script/QoRT.R &>> {log}; # needs QoRT package in R
        mkdir -p bam_qc/QoRTs_MultiPlot/details/;
        mv bam_qc/QoRTs_MultiPlot/plot-sample* bam_qc/QoRTs_MultiPlot/details/;

        D=bam_qc  ; # todo: more robust rule in case other bam_qc finishes faster (highly unlikely)
        rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ 
        """


rule bamCoverage:
    # osr
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bigWig/{sample}.{mode}.cpm.bw"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000,
    priority:
        0
    log:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.log"
    benchmark:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.tsv"
    # not installed in hand_sandbox yet
    run:
        if {wildcards.mode} == "strict":
            shell("""
            echo '{output}'
            which bamCoverage &> {log}
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            --outFileFormat bigwig --normalizeUsing CPM --binSize 10 \
            --minMappingQuality 20 &>> {log}
            """)
        else:
            shell("""
            echo '{output}'
            which bamCoverage &> {log}
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            --outFileFormat bigwig --normalizeUsing CPM --binSize 10  &>> {log}
            """)



rule feature_count:
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        gtf=GTF
    output:
        count="feature_count/counts.s{strand}.strict.txt" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt",
        summary="feature_count/counts.s{strand}.strict.txt.summary" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt.summary",
    priority:
        100
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    threads:
        4
    log:
        "log/feature_count/counts.s{strand}.log"
    benchmark:
        "log/feature_count/benchmark.s{strand}.tsv"
    run:
        if config["PAIR_END"]:
            if config["MODE"] == "strict":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                -p -B -C  \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            elif config["MODE"] == "liberal":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                -p \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            else:
                print ("MODE err, not liberal nor strict")
        else:
            if config["MODE"] == "strict":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            elif config["MODE"] == "liberal":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            else:
                print ("MODE err, not liberal nor strict")
            # -p: count Fragments rather than reads for Paired-end reads (remove this for SE data)
            # -C: exclude chimeric (most times, for cancer maybe not include this)
            # -d 50, -D 1000: including PE reads with fragment length in between, which is better than default 50-600
            # -T: num threads
            # -s: strand info, very important; use $i to perform all three possibilities, pick the correct one after counting
            # -Q: min MAPQ, if MAPQ from star, we need to be careful, because star ignores PE information, we might need to add addional step to rescue PE info. (https://github.com/alexdobin/STAR/issues/615)
            # -M: count multiple-mapping reads, based on NH, not useful for RNA-seq, may create misleading summary, by counting multi-mapping reads several times
            # -B: Only count read pairs that have both ends aligned.
            # --fracOverlap 0.2: 20% of read length
            # -â€“minOverlap 2: 2bp
            # Notes:
            # liberal overlap settings (--minOverlap 1 --fracOverlap 0) will give you more counted reads
            # samtools sorted bams are smaller, and faster to be counted, compared to unsorted bams/star sorted bams
            # star sorted bams are slow to count => use samtools sorted reads, delete star bam (set as temp)

if config['INTRON']:
    rule feature_count_gene_level:
        input:
            bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
            exon_counts="feature_count/counts.s{strand}.strict.txt" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt",
            gtf=GTF
        output:
            count='feature_count_gene_level/counts.s{strand}.gene_level.strict.txt' if config["MODE"] == "strict" else 'feature_count/counts.s{strand}.gene_level.liberal.txt',
            summary="feature_count_gene_level/counts.s{strand}.gene_level.strict.txt.summary" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.gene_level.liberal.txt.summary",
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 4000,
        threads:
            4
        log:
            "log/feature_count_gene_level/counts.s{strand}.gene_level.log"
        benchmark:
            "log/feature_count_gene_level/benchmark.s{strand}.gene_level.tsv"
        run:
            if config["PAIR_END"]:
                if config["MODE"] == "strict":
                    shell("""
                    echo '>>> Gene-Count mode:' >> {log}
                    featureCounts -a {input.gtf} -o {output.count} \
                    -T {threads} -t gene \
                    -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                    -p -B -C  \
                    -s {wildcards.strand} \
                    {input.bams} &>> {log}
                    """)
                elif config["MODE"] == "liberal":
                    shell("""
                    echo '>>> Gene-Count mode:' >> {log}
                    featureCounts -a {input.gtf} -o {output.count} \
                    -T {threads} -t gene \
                    -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                    -p \
                    -s {wildcards.strand} \
                    {input.bams} &>> {log}
                    """)
                else:
                    print ("MODE err, not liberal nor strict")
            else:
                if config["MODE"] == "strict":
                    shell("""
                    echo '>>> Gene-Count mode:' >> {log}
                    featureCounts -a {input.gtf} -o {output.count} \
                    -T {threads} -t gene \
                    -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                    -s {wildcards.strand} \
                    {input.bams} &>> {log}
                    """)
                elif config["MODE"] == "liberal":
                    shell("""
                    echo '>>> Gene-Count mode:' >> {log}
                    featureCounts -a {input.gtf} -o {output.count} \
                    -T {threads} -t gene \
                    -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                    -s {wildcards.strand} \
                    {input.bams} &>> {log}
                    """)
                else:
                    print ("MODE err, not liberal nor strict")

            shell("python script/get_max_of_featureCountsTable.py {input.exon_counts} {output.count}")


rule strand_detection:
    input:
        expand("feature_count/counts.s{strand}.strict.txt.summary", strand=STRAND) 
        if config["MODE"] == "strict" 
        else expand("feature_count/counts.s{strand}.liberal.txt.summary", strand=STRAND)
    output:
        "feature_count/counts.strict.txt" if config["MODE"] == "strict" else  "feature_count/counts.liberal.txt",
        "meta/strandness.detected.txt"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    log:
        "log/strand_detection.log"
    benchmark:
        "log/strand_detection.tsv"
    script:
        "script/strandness_detection.py" # todo: improvement needed



# rule SalmonTE_prep
if config['PAIR_END']:
    rule SalmonTE_prep:
        input:
            r1=expand("fastq/{sample}.R1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq/{sample}.R2.fastq.gz", sample=SAMPLES)    
        output:
            r1=expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000
        threads:
            1
        shell:
            """
            rm -rf fastq_salmon
            mkdir -p fastq_salmon
            cd fastq_salmon/
            for f in ../fastq/*gz;do ln -s $f;done
            rename .R1 _1 *
            rename .R2 _2 *
            cd ..
            """


# rule SalmonTE
if config['PAIR_END']:
    rule SalmonTE:
        # only start from Fastq
        input:
            r1=expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)
        output:
            "SalmonTE_output/EXPR.csv"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000,
        params:
            ref=config['TE_REFERENCE']
        threads:
            12
        log:
            'log/SalmonTE/salmonte_count.log'
        benchmark:
            'log/SalmonTE/salmonte_count.tsv'
        shell:
            """
            which python &> {log}
            python envs/SalmonTE/SalmonTE.py --version > versions.txt
            python envs/SalmonTE/SalmonTE.py quant \
            --reference={params.ref} --exprtype=count \
            --num_threads={threads} \
            {input.r1} {input.r2} >> {log} 2>&1
            """
else:
    rule SalmonTE:
        # only start from Fastq
        input:
            r1=expand("fastq/{sample}.fastq.gz", sample=SAMPLES),
        output:
            "SalmonTE_output/EXPR.csv"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000,
        params:
            ref=config['TE_REFERENCE']
        threads:
            12
        log:
            'log/SalmonTE/salmonte_count.se.log'
        benchmark:
            'log/SalmonTE/salmonte_count.se.tsv'
        shell:
            """
            which python &> {log}
            python envs/SalmonTE/SalmonTE.py quant \
            --reference={params.ref} --exprtype=count \
            --num_threads={threads} \
            {input.r1} >> {log} 2>&1
            """

rule Merge_TE_and_Gene:
    input:
        gene="feature_count/counts.strict.txt"
            if config['MODE'] == 'strict'
            else "feature_count/counts.liberal.txt",
        te="SalmonTE_output/EXPR.csv"
    output:
        "feature_count/TE_included.txt"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000,
    threads:
        1
    log:
        "log/merge_te_and_gene/merge_te_and_gene.log"
    benchmark:
        "log/merge_te_and_gene/merge_te_and_gene.tsv"
    shell:
        """
        python script/merge_featureCount_and_SalmonTE.py \
        {input.gene} {input.te} {output} > {log} 2>&1
        """


def DESeq2_input(config):
    if config["START"] in ["FASTQ", "BAM"]:
        if config["MODE"] == "strict":
            if config["TE_ANALYSIS"] and config["START"] == "FASTQ":
                return("feature_count/TE_included.txt")
            else:
                return("feature_count/counts.strict.txt")
        elif config["MODE"] == "liberal":
            if config["TE_ANALYSIS"] and config["START"] == "FASTQ":
                return("feature_count/TE_included.txt")
            else:
                return("feature_count/counts.liberal.txt")
        else:
            raise Exception("START config not valid")
    elif config["START"] in ["COUNT"]:
            return(config["COUNT_FILE"])
    else:
        return("placeholder" )


rule DESeq2:
# todo: use softlinked script/DESeq2.rmd, use softlinked example_data/ at COUNT-START
    input:
        DESeq2_input(config)
    output:
        "DESeq2/DESeq2.html",
        expand("DESeq2/rnk/{contrast}.rnk", contrast=CONTRASTS_DE)
    container:
        "envs/hand_sandbox.simg"
    # conda:
    #     "envs/r.yaml" # installing R via conda is problematic
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000,
    params:
        rmd="'script/DESeq2.Rmd'",
        fdr=MAX_FDR,
        lfc=MIN_LFC,
        independentFilter=config["independentFilter"],
        cooksCutoff=config["cooksCutoff"],
        anno_tab=ANNO_TAB,
        meta=config["META"],
        cont=config["CONTRAST_DE"],
        o="'../DESeq2/DESeq2.html'"
    priority: 
        100
    threads:
        1
    log:
        "log/DESeq2/DESeq2.log"
    benchmark:
        "log/DESeq2/DESeq2.tsv"
    shell:
        #Rscript -e rmarkdown::render({params.rmd})
        'mkdir -p DESeq2; ' 
        'cp script/DESeq2.Rmd DESeq2; '
        'Rscript -e "rmarkdown::render({params.rmd}, \
        params=list(max_fdr={params.fdr}, \
        min_lfc={params.lfc}, \
        cookscutoff={params.cooksCutoff}, \
        indfilter={params.independentFilter}, \
        countFile=\'../{input}\', \
        annoFile=\'{params.anno_tab}\', \
        metaFile=\'../{params.meta}\', \
        contrastFile=\'../{params.cont}\'           ), \
        output_file={params.o})" > {log} 2>&1 ;'
        'D=DESeq2; rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};'


rule GSEA:
    """
    config[START] == RNK, then find config[RNKS], else, find {contrast} 
    config[RNKS] contains the full name of the rank file (xlsx or txt), must be put in ./meta/ folder
    rnk file must be named xxx.rnk.txt when feeded to GSEA

    # todo: 
    [x] no svg if nplots > 200 (yes)
    [] 
    """
    input: 
        deseq2= "Workflow_DAG.all.svg" # dummy assume existing
            if config["START"] == "RNK"
            else "DESeq2/DESeq2.html" , # DESEq2, then GSEA
        rnk="meta/{contrast}"  # contrast just a name, alias for rnk, see rule targets
            if config["START"] == "RNK"
            else "DESeq2/rnk/{contrast}.rnk",
        db="meta/gsea_db/{db}"
    output:
#        touch("log/gsea/{contrast}/{db}.finished"),  # todo: premature killed jobs create junk sub folders, confusing to users
        "gsea/{contrast}/{db}.GseaPreranked/index.html"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 12000,
    params:
        svg=config["GSEA_PLOT_SVG"] if config["GSEA_NPLOTS"] <= 200 else False,  # no svg if nplots > 200 (to save time and space) # todo: move this to front end, when making workflow portable
        nplot=config["GSEA_NPLOTS"],
        label_db=lambda wildcards: wildcards["db"][:-12],
        rnk_flat_file= lambda wildcards: re.sub(".rnk.xlsx$", ".rnk.txt", wildcards["contrast"]) \
                    if wildcards["contrast"].endswith(".rnk.xlsx") \
                    else wildcards["contrast"]
    threads:
        1
    log:
        "log/gsea/{contrast}.{db}.log"
    benchmark:
        "log/gsea/{contrast}.{db}.tsv"
    envmodules:
        "openjdk/11+28"
    priority:
        100
    shell:
        """
        echo {input.rnk} {params.label_db} > {log}

        mkdir -p gsea && mkdir -p gsea/{wildcards.contrast} &>> {log}
        rm -rf gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked.*/  # avoid confusion
        rm -rf gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/  # avoid dir structure mistake
        rm -rf gsea/{wildcards.contrast}/error_{wildcards.db}*GseaPreranked*/  # avoid confusion of temp files in multiple run attempts

        python script/fix_gsea_input.py {input.rnk} &>> {log}  # fix gene symbol error; change test.xlsx to test.rnk.txt
        
        which java &>> {log}

        envs/GSEA_4.0.3/gsea-cli.sh GSEAPreranked \
        -gmx {input.db} -rnk meta/{params.rnk_flat_file} -rpt_label {wildcards.db} \
        -norm meandiv -nperm 1000  -scoring_scheme classic \
        -create_svgs {params.svg} -make_sets true  -rnd_seed timestamp -zip_report false \
        -set_max 15000 -set_min 15 \
        -plot_top_x {params.nplot} -out ./gsea/{wildcards.contrast} &>> {log}

        # rename
        mv gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked.*/ gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/  &>> {log}   # if gsea fails, might still have problems, not sure

        cp envs/GSEA_MSigDB_ReadMe.html gsea/ &>> {log}
        # compress
        D=gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked
        rm -f $D.zip && [ -d $D ] && zip -rq $D.zip $D/ &>> {log}
        """

# todo: if gsea/contrast/err_{db}, touch gsea/contrast/db.finished

rule read_length_detection_bam:
    input:
        expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES)
            if config['START'] in ["FASTQ", "BAM"]
            else "BAM file not provided. AND can't be generated from FASTQ, because FASTQ not provided"
    output:
        "meta/read_length.txt"
    resources:
        mem_mb="1000",
    threads:
        1
    log:
        "log/read_length_detection_bam/read_length_detection_bam.log"
    benchmark:
        "log/read_length_detection_bam/read_length_detection_bam.tsv"
    run:
        import re
        import statistics
        lengths = []
        for f in input:
            p = re.compile("average length:\s*(\d*)")
            for i, line in enumerate(open(f, "r")):
                for match in re.finditer(p, line):
                    lengths.append(match.group(1))
        lengths = list(map(int, lengths))
        print("read lengths detected from BAM:", lengths,"\n")
        if(len(set(lengths)) < 1):
            sys.exit("read lengths detection from BAM failed")
        if(len(set(lengths)) > 1):
            print("Not all fastq files have the same length, will give rMATS the median length")
        
        median_length = int(statistics.median(lengths))
        print(output[0])
        with open(output[0], "w") as out:
            out.write(str(median_length))

def get_strandness (x):
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = (config['RMATS_STRANDNESS'][int(res.group(1))])
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)


def read_length(x):
    try:
        with open("meta/read_length.txt", "r") as file:
            txt = file.readline()
        #print("read length", int(txt))
        return (int(txt))
    except FileNotFoundError:
        print("meta/read_length.txt not found in dry run, will be found in real run")
        return (None)


rule rMATS:
    #todo: update to docker: simple to install, faster, recommended on website
    input:
        b1=lambda wildcards: B1S[int(wildcards['ascn'])-1],
        b2=lambda wildcards: B2S[int(wildcards['ascn'])-1],
        gtf=GTF, 
        length_file="meta/read_length.txt" , 
        strand_file="meta/strandness.detected.txt",
    output:
        "rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt"
    envmodules:
        "rMATS/4.1.0 "
        "gcc/8.1.0 " # must have space
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,  # todo reduce
    threads:
        4
    params:
        b1=lambda wildcards: B1[int(wildcards['ascn'])-1], 
        b2=lambda wildcards: B2[int(wildcards['ascn'])-1], 
        type="paired" if config['PAIR_END'] else "single", 
        analysis="P" if config['PAIR_END'] else "U", 
        length=read_length, 
        strandness=get_strandness,
        MAX_FDR=MAX_FDR
    log:
        "log/rMATS/rMATS.{ascn}.log"
    benchmark:
        "log/rMATS/rMATS.{ascn}.tsv"
    shell:
        """
        rm -rf rMATS.{wildcards.ascn}
        mkdir -p rMATS.{wildcards.ascn}/
        mkdir -p rMATS.{wildcards.ascn}/params/


        echo {params.b1} > rMATS.{wildcards.ascn}/params/b1.txt
        echo {params.b2} > rMATS.{wildcards.ascn}/params/b2.txt

        which python &> {log}

        # todo: remove hardcode path
        python /share/pkg/rMATS/4.1.0/rmats.py  \
        --b1 rMATS.{wildcards.ascn}/params/b1.txt \
        --b2  rMATS.{wildcards.ascn}/params/b2.txt \
        --gtf {input.gtf} \
        -t {params.type} \
        --readLength {params.length} \
        --variable-read-length \
        --libType {params.strandness} \
        --nthread {threads} \
        --tstat {threads} \
        --cstat 0.2 \
        --od rMATS.{wildcards.ascn}/output/ \
        --tmp rMATS.{wildcards.ascn}/tmp/  &>> {log}

        cp envs/rMATS_ReadMe.html rMATS.{wildcards.ascn}/output/  &>>{log}

        rm -rf rMATS.{wildcards.ascn}/tmp/   &>> {log}

        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/ &>> {log}
        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/  &>> {log}
        mkdir -p rMATS.{wildcards.ascn}/output/fromGTF/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/*JCEC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/*JC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/fromGTF*txt  rMATS.{wildcards.ascn}/output/fromGTF/ &>> {log}
        rm -f rMATS.{wildcards.ascn}/output/*raw.input* &>> {log}

        D=rMATS.{wildcards.ascn};
        rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};
        """


DEXSeq_GFF = config['GTF']+".dexseq.gff"

rule DEXSeq_Prep_GFF:
    input:
        config['GTF']
    output:
        DEXSeq_GFF
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        1
    log:
        "log/DEXSeq/prep_gff.log"
    benchmark:
        "log/DEXSeq/prep_gff.tsv"
    shell:
        "python script/dexseq_prepare_annotation.py -r no {input} {output} &> {log}"


def get_strandness_for_dexseq (x):
    book = {0 : '-s no', 1 : '-s yes', 2 : '-s reverse'}
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = book[int(res.group(1))]
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)

rule prep_count:
    input:
        bam="sorted_reads/{sample}.bam",
        strandFile="meta/strandness.detected.txt", 
        gff=DEXSeq_GFF
    output:
        "DEXSeq_count/{sample}_count.txt"
    params:
        strand=get_strandness_for_dexseq,
        readType=('-p yes -r pos'
            if config['PAIR_END']
            else ' '), 
        MAPQ=10
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 16000
    log:
        "log/DEXSeq/prep_count.{sample}.log"
    benchmark:
        "log/DEXSeq/prep_count.{sample}.tsv"
    #conda:
    #    "envs/dexseq.yaml"
    shell:
        "python script/dexseq_count.py -f bam -a {params.MAPQ} {params.readType} {params.strand} {DEXSeq_GFF} {input.bam} {output}"  
        # todo: maybe -r name (sort by name) is faster


rule DEXSeq:
    input:
        count_files = expand("DEXSeq_count/{sample}_count.txt", sample = SAMPLES),
        meta=config["META"],
        contrast=config["CONTRAST_AS"],
        gffFile=config['GTF']+".dexseq.gff",
        strandness="meta/strandness.detected.txt"
    output:
        "DEXSeq/contrast{ascn}/contrast{ascn}.RData"
    conda:
        "envs/dexseq.yaml"
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 3000
    threads:
        12
    priority:
        10
    params:
        rmd="'script/dexseq.r'",
        annoFile=config["ANNO_TAB"],
    log:
        "log/DEXSeq/DEXSeq.{ascn}.log"
    benchmark:
        "log/DEXSeq/DEXSeq.{ascn}.tsv"
    shell:
        """
        mkdir -p DEXSeq 
        which Rscript &> {log}
        cp script/dexseq.r DEXSeq/ &>> {log}
        Rscript script/dexseq.r {input.meta} {input.contrast} {input.gffFile} {params.annoFile} {MAX_FDR} {MIN_LFC} {threads} {MIN_GENE_COUNT} {wildcards.ascn} &>> {log}
        D=DEXSeq/contrast{wildcards.ascn}
        rm -f $D.zip && [ -d $D ] && zip -rq $D.zip $D/ &>> {log}
        """

rule genome_faidx:
    input:
        config['GENOME']
    output:
        config['GENOME']+'.fai'
    resources:
        mem_mb=lambda wildcards, attempt: attempt *4000
    threads:
        1
    log:
        "log/genome_faidx/faidx.log"
    shell:
        "samtools faidx {input}  &> {log}"


rule GATK_CreateSequenceDictionary:
    input:
        config['GENOME'],
    output:
        re.sub( "fa$", "dict", config['GENOME'])
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    envmodules:
        "java/1.8.0_171"
    threads:
        1
    log:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.log"
    benchmark:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.tsv"
    shell:
        "./envs/gatk-4.1.8.1/gatk CreateSequenceDictionary -R {input} &> {log}"


rule GATK_ASEReadCounter:
    input:
        genome=config['GENOME'],
        genome_idx=config['GENOME']+'.fai',
        genome_ref=re.sub( "fa$", "dict", config['GENOME']),
        bam="sorted_reads/{sample}.bam",
        vcf=config['VCF']
    output:
        table="GATK_ASEReadCounter/{sample}.table"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    envmodules:
        "java/1.8.0_171"
    threads:
        1
    log:
        "log/GATK_ASEReadCounter/{sample}.log"
    benchmark:
        "log/GATK_ASEReadCounter/{sample}.tsv"
    shell:
        "./envs/gatk-4.1.8.1/gatk ASEReadCounter -R {input.genome} -I {input.bam} -V {input.vcf} -O {output} \
        --min-depth-of-non-filtered-base 10 --min-mapping-quality 15 --min-base-quality 20 \
        &> {log};"  # todo: more thoughts on detailed parameters, e.g.    -drf DuplicateRead -U ALLOW_N_CIGAR_READS
        "D=GATK_ASEReadCounter; "
        "rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};"


rule hisat2_build:
    """
    guided by GTF
    """
    input:
        genome=config['GENOME'],
        gtf=config['GTF']
    output:
        ss=config['GTF']+"ss",
        exon=config['GTF']+"exon",
        index=config['GENOME'] + ".1.ht2"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        4
    log:
        "log/hisat2_build/hisat2_build.log"
    benchmark:
        "log/hisat2_build/hisat2_build.tsv"
    shell:
        """
        hisat2_extract_splice_sites.py -v {input.gtf} > {output.ss} 2> {log}
        hisat2_extract_exons.py -v {input.gtf} > {output.exon} 2>> {log}
        hisat2-build -p {threads} {input.genome} {input.genome} --ss {output.ss} --exon {output.exon} 2>> {log}
        """


def get_strandness_for_hisat2_PE (x):
    book = {0 : 'unstranded', 1 : 'FR', 2 : 'RF'}
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = book[int(res.group(1))]
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)

def get_strandness_for_hisat2_SE (x):
    book = {0 : 'unstranded', 1 : 'F', 2 : 'R'}
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = book[int(res.group(1))]
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)


if config['PAIR_END']:
    rule hisat2:
        input:
            genome=config['GENOME'],
            index=config['GENOME'] + ".1.ht2",
            R1="fastq/{sample}.R1.fastq.gz",
            R2="fastq/{sample}.R2.fastq.gz",
        output:
            bam="hisat2/{sample}.bam",
            bai="hisat2/{sample}.bam.bai"
        params:
            strand=get_strandness_for_hisat2_PE
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 2000
        threads:
            8
        log:
            "log/hisat2/{sample}.hisat2.log"
        benchmark:
            "log/hisat2/{sample}.hisat2.tsv"
        shell:
            """
            mkdir -p hisat2
            hisat2 -x {input.genome} -p {threads} --dta-cufflinks \
            -1 {input.R1} -2 {input.R2} \
            --rna-strandness {params.strand} 2> {log} | \
            samtools sort -@ 2 -o {output.bam}
            samtools index {output.bam}
            """
else:
    rule hisat2:
        input:
            index=config['GENOME'] + ".1.ht2",
            U="fastq/{sample}.fastq.gz"
        output:
            bam="hisat2/{sample}.bam",
            bai="hisat2/{sample}.bam.bai"
        params:
            strand=get_strandness_for_hisat2_SE
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 2000
        threads:
            8
        log:
            "log/hisat2/{sample}.hisat2.log"
        benchmark:
            "log/hisat2/{sample}.hisat2.tsv"
        shell:
            """
            mkdir -p hisat2
            hisat2 -x {input.genome} -p {threads} --dta-cufflinks \
            -U {input.U} \
            --rna-strandness {params.strand} 2> {log} | \
            samtools sort -@ 2 -o {output.bam}
            samtools index {output.bam}
            """

def get_strandness_for_stringtie (x):
    book = {0 : ' ', 1 : '--fr', 2 : '--rf'}
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = book[int(res.group(1))]
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)

rule stringtie:  # todo: group level
    input:
        bam="hisat2/{sample}.bam",
        gtf=config['GTF'],
        strandness='meta/strandness.detected.txt'
    output:
        "stringtie/{sample}.stringtie.gtf"
    params:
        strand=get_strandness_for_stringtie
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        4
    log:
        "log/stringtie/{sample}.stringtie.log"
    benchmark:
        "log/stringtie/{sample}.stringtie.tsv"
    shell:
        """
        mkdir -p stringtie
        stringtie -G {input.gtf} {params.strand} -o {output} {input.bam} 2> {log}
        """

rule stringtie_merge:
    input:
        stringties=expand("stringtie/{sample}.stringtie.gtf", sample=SAMPLES),
        gtf=config['GTF']
    output:
        "stringtie/stringtie.merged.gtf"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        4
    log:
        "log/stringtie/stringtie.merged.log"
    benchmark:
        "log/stringtie/stringtie.merged.tsv"
    shell:
        """
        stringtie --merge -G {input.gtf} -o {output} {input.stringties} 2> {log}
        """


rule create_dag:
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000,
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    envmodules:
        "graphviz/2.26.0"
    shell:
        "snakemake --dag targets > dag 2> {log};"
        "which dot &>> {log};"
        "cat dag|dot -Tsvg > {output} 2>> {log}"


# rule organize_results:
#     input:
#         mapping=expand("mapped_reads/{sample}.bam", sample=config['SAMPLES']),
#     output:
#         touch("log/oranize_results.finished")
#     params:
#         mem_mb="1000"
#     log:
#         "log/organize_results.log"
#     shell:
#         "gzip -f mapped_reads/*.Unmapped.out.mate*"
# 

rule reset:
    shell:
        """
        echo 'deleting files..'
        rm -rf fastqc/ bam_qc/ mapped_reads/ sorted_reads/ bam_qc/ bigWig/ \
        feature_count/ fastq_salmon SalmonTE_output/ DESeq2/ gsea/ gsea_compressed/ \
        GATK_ASEReadCounter/ DEXSeq_count/  DEXSeq/ rMATS.*/ \
        _STARgenome _STARtmp \
        feature_count_gene_level hisat2 stringtie \
        lsf.log Log.out nohup.out report.log report.html  dag Workflow_DAG.all.svg \
        meta/strandness.detected.txt  meta/decoder.txt meta/read_length.txt
        echo 'unlocking dir..'
        snakemake -j 1 --unlock
        """

