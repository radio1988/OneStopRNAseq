configfile: "config.yaml"


SAMPLES=config['SAMPLES']
GENOME=config["GENOME"]
INDEX=config["INDEX"]
GTF=config["GTF"]
ANNO_TAB=config["ANNO_TAB"]
STRAND=config["STRAND"]
MODE=config["MODE"]
MAX_FDR=config["MAX_FDR"]
MIN_LFC=config["MIN_LFC"]

# priority 0-100, default 0



# get CONTRASTS from meta/contrast.xlsx
import pandas as pd
df=pd.read_excel(config["CONTRAST"])
CONTRASTS = []
for j in range(df.shape[1]):
    CONTRASTS.append(df.iloc[0,j] + "_vs_" + df.iloc[1,j])
#print(CONTRASTS)

# load modules (have to use """, to keep in one block)
# - alias does not work, have to use $samstat
shell.prefix("""
            #source /home/rl44w/.bash_profile
            #module load fastqc/0.11.5
            #module load java/1.8.0_77
            # module load star/2.5.3a
            module load singularity/singularity-current
            #module load deeptools/3.0.2_py36_0
            # conda activate osr  # have to create conda env osr first

            samstat="singularity exec envs/hand_sandbox.simg samstat"
            #samtools="singularity exec /project/umw_mccb/singularity/hand_sandbox.simg samtools"

            """)

# Requirements
# inputs in ./fastq/xxx.{fastq,fq}.gz
# named as {sample}.{R1,R2}.{fastq,fq}.gz

# SnakeMake Coding Notes:
# input don't have to be used, just for draw nice DAG
# 07/10/2019 randomized primary alignment


rule targets:
    input:
        # 1. everything listed here will be produced by the pipeline
        # 2. feed {sample}
        fastqc=("fastqc/multiqc_report.html" 
            if config["START"] == "FASTQ" 
            else "Workflow_DAG.all.svg"),
        bam_qc=(expand("bam_qc/samstat/{sample}.bam.samstat.html", sample=SAMPLES) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),
        bamCoverage=(expand("bigWig/{sample}.{mode}.cpm.bw", sample=SAMPLES, mode=MODE) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),
        feature_count_strict=(expand("feature_count/counts.s{strand}.strict.txt", strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"]  and config["MODE"] == "strict"
            else "Workflow_DAG.all.svg"),
        feature_count_liberal=(expand("feature_count/counts.s{strand}.liberal.txt", strand=STRAND)       
            if config["START"] in ["FASTQ", "BAM"]  and config["MODE"] == "liberal"
            else "Workflow_DAG.all.svg"),
        DESeq2="DESeq2/DESeq2.html",
        GSEA = expand("gsea/{contrast}/{db}.finished", contrast=CONTRASTS, db=config["GSEA_DBS"]),
        dag="Workflow_DAG.all.svg", # create DAG
    params:
        mem_mb="1000"


rule fastqc:
    # don't need input, if you agree on not checking them
    # without output, output will not be created
    input:
        r1="fastq/{sample}.R1.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz",
        r2="fastq/{sample}.R2.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz", # trick snakemake to skip r2
    output:
        r1="fastqc/details/{sample}.R1_fastqc.html" 
            if config["PAIR_END"] 
            else "fastqc/details/{sample}_fastqc.html",
        r2="fastqc/details/{sample}.R2_fastqc.html" 
            if config["PAIR_END"] 
            else "fastqc/details/{sample}_fastqc.html", # trick snakemake to skip r2
    priority:
        10
    params:
        mem_mb="2000"
    threads:
        1
    log:
        "log/fastqc/{sample}.log"
    benchmark:
        "log/fastqc/{sample}.tsv"
    run:
        shell("mkdir -p fastqc && mkdir -p fastqc/details")
        if config["PAIR_END"]:
            shell("fastqc -t {threads} {input} -o fastqc/details &> {log}")
        else:
            shell("fastqc -t {threads} {input.r1} -o fastqc/details &> {log}")


rule multiqc:
    input:
        r1=expand("fastqc/details/{sample}.R1_fastqc.html", sample=SAMPLES) 
            if config["PAIR_END"] 
            else expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES),
        r2=expand("fastqc/details/{sample}.R2_fastqc.html", sample=SAMPLES) 
            if config["PAIR_END"] 
            else expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES), # trick snakemake to skip r2
    priority:
        10
    params:
        mem_mb="2000"
    threads:
        1
    output:
        "fastqc/multiqc_report.html"
    log:
        "log/multiqc/multiqc.log"
    benchmark:
        "log/multiqc/multiqc.tsv"
    shell:
        "multiqc fastqc/details -o fastqc &> {log}"


rule star_idx:
    input:
        fa=GENOME,
        gtf=GTF,
    output:
        INDEX+"/SA"
    params:
        mem_mb="6000"
    threads:
        6
    log:
        "log/star_idx/star_idx.log"
    benchmark:
        "log/star_idx/star_idx.tsv"
    shell:
        """
        mkdir -p {INDEX}

        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {INDEX} \
        --genomeFastaFiles {input.fa} \
        --sjdbGTFfile {input.gtf} &> {log}

        mv Log.out {INDEX}
        """


rule star_map:
    input:
        index=INDEX+"/SA",
        gtf=GTF,
        r1="fastq/{sample}.R1.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz",
        r2="fastq/{sample}.R2.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz", # trick snakemake to skip r2
    output:
        bam="mapped_reads/{sample}.bam"
    params:
        mem_mb="3000",  # todo auto adjust based on {threads}
        reads="fastq/{sample}.R1.fastq.gz fastq/{sample}.R2.fastq.gz" if config["PAIR_END"] else "fastq/{sample}.fastq.gz",
    threads:
        12
    log:
        "log/mapped_reads/{sample}.star.log"
    benchmark:
        "log/mapped_reads/{sample}.star.tsv"
    run:
        # align; rename
        shell("""STAR --runThreadN {threads} \
        --genomeDir {INDEX} \
        --sjdbGTFfile {input.gtf} \
        --readFilesCommand zcat \
        --readFilesIn {params.reads} \
        --outFileNamePrefix mapped_reads/{wildcards.sample}. \
        --outFilterType BySJout \
        --outMultimapperOrder Random \
        --outFilterMultimapNmax 200 \
        --alignSJoverhangMin 8 \
        --alignSJDBoverhangMin 3 \
        --outFilterMismatchNmax 999 \
        --outFilterMismatchNoverReadLmax 0.05 \
        --alignIntronMin 20 \
        --alignIntronMax 1000000 \
        --alignMatesGapMax 1000000 \
        --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
        --outSAMstrandField None \
        --outSAMtype BAM Unsorted \
        --quantMode GeneCounts \
        --outReadsUnmapped Fastx \
        &> {log}

        mv mapped_reads/{wildcards.sample}*.out.bam mapped_reads/{wildcards.sample}.bam
        
        gzip -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*
        """)

# rule star_align:
#     input:



rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    # conda:
    #   "envs/samtools.yaml"
    # group:
    #     "sort"
    params:
        mem_mb="3000"
    threads:
        2
    log:
        "log/samtools_sort/{sample}.sort.log"
    benchmark:
        "log/samtools_sort/{sample}.sort.tsv"
    shell:
        "samtools sort -@ {threads} -m 3G {input} -o sorted_reads/{wildcards.sample}.bam &> {log}"


rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    conda:
      "envs/samtools.yaml"
    # group:
    #     "sort"
    params:
        mem_mb="3000"
    threads:
        1
    log:
        "log/samtools_index/{sample}.index.log"
    benchmark:
        "log/samtools_index/{sample}.index.tsv"
    shell:
        "samtools index -@ {threads} {input} &> {log}"


rule bam_qc:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bam_qc/samstat/{sample}.bam.samstat.html"
    params:
        mem_mb="4000"
    threads:
        4
    log:
        idxstats="log/bam_qc/idxstats/{sample}.idxstats.log",
        flagstat="log/bam_qc/flagstat/{sample}.flagstat.log",
        stats="log/bam_qc/stats/{sample}.stats.log",
        samstat="log/bam_qc/samstat/{sample}.samstat.log",
    benchmark:
        "log/bam_qc/benchmark/{sample}.bam_qc.tsv",
    shell:
        """
        mkdir -p bam_qc 
        mkdir -p bam_qc/idxstats
        mkdir -p bam_qc/flagstat
        mkdir -p bam_qc/stats
        mkdir -p bam_qc/samstat
        samtools idxstats {input.bam} > bam_qc/idxstats/{wildcards.sample}.idxstats.txt 2> {log.idxstats} &
        samtools flagstat {input.bam} > bam_qc/flagstat/{wildcards.sample}.flagsat.txt 2> {log.flagstat} &
        samtools stats {input.bam} > bam_qc/stats/{wildcards.sample}.stats.txt 2> {log.stats} &
        $samstat {input.bam} && mv sorted_reads/{wildcards.sample}*.samstat.html bam_qc/samstat 2> {log.samstat}
        """


rule bamCoverage:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bigWig/{sample}.{mode}.cpm.bw"
    threads:
        8
    params:
        mem_mb="4000",
        common_strict="--outFileFormat bigwig --normalizeUsing CPM --minMappingQuality 20 --binSize 10 ",
        common_liberal="--outFileFormat bigwig --normalizeUsing CPM --binSize 10 ",
    priority:
        0
    log:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.log"
    benchmark:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.tsv"
    run:
        if {wildcards.mode} == "strict":
            shell("""
            echo '{output}'

            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            {params.common_strict} &> {log}
            """)
        else:
            shell("""
            echo '{output}'
            
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            {params.common_liberal} &> {log}
            """)



rule feature_count:
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        gtf=GTF
    output:
        count="feature_count/counts.s{strand}.strict.txt" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt",
        summary="feature_count/counts.s{strand}.strict.txt.summary" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt.summary"
    # conda:
    #   "envs/subread.yaml"  # not compatible with 'run'
    priority:
        100
    params:
        mem_mb="4000",
        common_uniq="-g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 ",
        #common_mult="-g gene_id -M --minOverlap 1 --fracOverlap 0",
        common_mult="-g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0",
        pe_strict="-p -B -C ",
        pe_liberal="-p",
    threads:
        4
    log:
        pe_strict="log/feature_count/counts.s{strand}.pe_strict.log",
        pe_liberal="log/feature_count/counts.s{strand}.pe_liberal.log",
        se_uniq="log/feature_count/counts.s{strand}.uniq.se.log",
        se_mult="log/feature_count/counts.s{strand}.mult.se.log",
    benchmark:
        "log/feature_count/benchmark.s{strand}.tsv"

    run:
        if config["PAIR_END"]:
            if config["MODE"] == "strict":
                shell("""
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                {params.common_uniq} {params.pe_strict} \
                -s {wildcards.strand} \
                {input.bams} &> {log.pe_strict} &
                """)
            elif config["MODE"] == "liberal":
                shell("""
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                {params.common_mult} {params.pe_liberal} \
                -s {wildcards.strand} \
                {input.bams} &> {log.pe_liberal}
                """)
            else:
                print ("MODE err, not liberal nor strict")
        else:
            shell("""
            # strict
            featureCounts -a {input.gtf} -o {output.count} \
            -T {threads} \
            {params.common_uniq} \
            -s {wildcards.strand} \
            {input.bams} &> {log.se_uniq} &
            # liberal
            featureCounts -a {input.gtf} -o {output.count} \
            -T {threads} \
            {params.common_mult} \
            -s {wildcards.strand} \
            {input.bams} &> {log.se_mult}
            """)

# -p: count Fragments rather than reads for Paired-end reads (remove this for SE data)
# -C: exclude chimeric (most times, for cancer maybe not include this)
# -d 50, -D 1000: including PE reads with fragment length in between, which is better than default 50-600
# -T: num threads
# -s: strand info, very important; use $i to perform all three possibilities, pick the correct one after counting
# -Q: min MAPQ, if MAPQ from star, we need to be careful, because star ignores PE information, we might need to add addional step to rescue PE info. (https://github.com/alexdobin/STAR/issues/615)
# -M: count multiple-mapping reads, based on NH, not useful for RNA-seq, may create misleading summary, by counting multi-mapping reads several times
# -B: Only count read pairs that have both ends aligned.
# --fracOverlap 0.2: 20% of read length
# -â€“minOverlap 2: 2bp
# Notes:
# liberal overlap settings (--minOverlap 1 --fracOverlap 0) will give you more counted reads
# samtools sorted bams are smaller, and faster to be counted, compared to unsorted bams/star sorted bams
# star sorted bams are slow to count => use samtools sorted reads, delete star bam (set as temp)


rule auto_detect_strand:
    input:
        expand("feature_count/counts.s{strand}.strict.txt.summary", strand=STRAND) 
        if config["MODE"] == "strict" 
        else expand("feature_count/counts.s{strand}.liberal.txt.summary", strand=STRAND)
    output:
        "feature_count/counts.strict.txt" if config["MODE"] == "strict" else  "feature_count/counts.liberal.txt"
    threads:
        1
    params:
        mem_mb=1000
    log:
        "log/feature_count/auto_detect_strand.log"
    script:
        "script/auto_detect_strand.py" # todo: improvement needed





rule DESeq2:
    input:
        "feature_count/counts.strict.txt" if config["MODE"] == "strict" else  "feature_count/counts.liberal.txt" # todo: use {mode} wildcards
    output:
        "DESeq2/DESeq2.html",
        expand("DESeq2/rnk/{contrast}.rnk", contrast=CONTRASTS)
    conda:
        "envs/r.yaml"
    params:
        mem_mb="8000",
        rmd="'script/DESeq2.Rmd'",
        fdr=MAX_FDR,
        lfc=MIN_LFC,
        anno_tab=ANNO_TAB,
        meta=config["META"],
        cont=config["CONTRAST"],
        o="'../DESeq2/DESeq2.html'"
    priority: 
        100
    threads:
        1
    log:
        "log/DESeq2/DESeq2.log"
    benchmark:
        "log/DESeq2/DESeq2.tsv"
    shell:
        #Rscript -e rmarkdown::render({params.rmd})
        'Rscript -e "rmarkdown::render({params.rmd}, \
        params=list(max_fdr={params.fdr}, min_lfc={params.lfc}, \
        countFile=\'../{input}\', \
        annoFile=\'{params.anno_tab}\', \
        metaFile=\'../{params.meta}\', \
        contrastFile=\'../{params.cont}\'           ), \
        output_file={params.o})" '





rule GSEA:
    input: 
        deseq2="DESeq2/DESeq2.html", # to keep ordering
        rnk="DESeq2/rnk/{contrast}.rnk",
        db="script/gsea_db/{db}"
    output:
        touch("gsea/{contrast}/{db}.finished")  # todo: premature killed jobs create junk sub folders, confusing to users
        #directory("gsea/{contrast}/{db}")
        #directory(lambda wildcards: "gsea/"+wildcards['contrast']+wildcards['db'][:-12])
    params:
        mem_mb="6200",
        svg=config["GSEA_PLOT_SVG"],
        nplot=config["GSEA_NPLOTS"],
        label_db=lambda wildcards: wildcards["db"][:-12]
    threads:
        2
    log:
        "log/gsea/{contrast}.{db}.log"
    benchmark:
        "log/gsea/{contrast}.{db}.tsv"
    # run:
    #     print('test')
    run:
        shell("echo {params.label_db}")
        shell("mkdir -p gsea && mkdir -p gsea/{wildcards.contrast}")
        shell("""#module purge;             module load  openjdk/11+28; 
            script/GSEA_4.0.3/gsea-cli.sh GSEAPreranked \
            -gmx {input.db} -rnk {input.rnk} -rpt_label {wildcards.db} \
            -norm meandiv -nperm 1000  -scoring_scheme classic \
            -create_svgs {params.svg} -make_sets true  -rnd_seed timestamp -zip_report false \
            -set_max 15000 -set_min 15 \
            -plot_top_x {params.nplot} -out ./gsea/{wildcards.contrast} &> {log}""")

# todo: if gsea/contrast/err_{db}, touch gsea/contrast/db.finished

rule test:
    input:
        "config.yaml"
    output:
        "test.out"
    params:
        cmd=config['MAX_FDR']
    shell:
        'echo test'
        'echo {params.cmd}'



# rule fpkm_tpm:
#     input:
#         count="feature_count/counts.gene_id.s{strand}.txt",
#         anno_tab=config['ANNO_TAB']
#     output:
#         fpkm="fpkm_tpm/strand_{strand}/FPKM.xlsx",
#         tpm='fpkm_tpm/strand_{strand}/TPM.xlsx',
#     params:
#         mem_mb="4000"
#     threads:
#         2
#     log:
#         "log/fpkm_tpm/fpkm_tpm.{strand}.log"
#     shell:
#         """
#         Rscript {config[FPKM]} {input.count} {input.anno_tab} fpkm_tpm/strand_{wildcards.strand} &> {log}
#         """
#         # {config[FPKM]} without quotes


rule create_dag:
    params:
        mem_mb="1000"  
        # every job has to have this defined 
        # to use snakemake --cluster 'bsub -q short -R "rusage[mem_mb={params.mem_mb}]" -n {threads}'
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    shell:
        "snakemake --dag targets | dot -Tsvg > {output} 2> {log}"

rule reset:
    shell:
        "rm -rf fastqc/ bigWig/ bam_qc/ mapped_reads/ sorted_reads/ create_dag/ \
        lsf.log Log.out Workflow_DAG.all.svg report.html feature_count log nohup.out DESeq2 gsea .snakemake"

